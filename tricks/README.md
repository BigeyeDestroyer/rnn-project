### Introduction 
This folder is for all the popular tricks implemented in RNN such as dropout, weight regularization, batch-normalize, etc. 

### Popular tricks
1. Dropout
	- [BayesianRNN][1]
	- [code][2]

2. Weight Normalization
	- [Accelerate DNN][3]
	- [code][4]

3. Initialization
	- [good init][5]
	- [code][6]

4. Boosting
	- [XGBoost][7]
	- [code][8]

5. lime
	- Explaining the predictions of any mahine learning classifier. 
	- [code][9]

6. persistent RNN
	- A technical report that accelerate RNN training with 30x speed. 
	- [report][10]

7. [compare DNN with SVM or random forest][11]

8. [Escape from saddle point][12]

[1]:	http://arxiv.org/abs/1512.05287
[2]:	https://github.com/yaringal/BayesianRNN
[3]:	http://arxiv.org/abs/1602.07868
[4]:	https://github.com/TimSalimans/weight_norm
[5]:	http://arxiv.org/abs/1511.06422
[6]:	https://github.com/ducha-aiki/LSUVinit
[7]:	http://arxiv.org/pdf/1603.02754.pdf
[8]:	https://github.com/dmlc/xgboost
[9]:	https://github.com/marcotcr/lime
[10]:	https://svail.github.io/persistent_rnns/
[11]:	https://github.com/rasbt/python-machine-learning-book/blob/master/faq/deeplearn-vs-svm-randomforest.md
[12]:	http://www.offconvex.org/2016/03/22/saddlepoints/