### Introduction 
This folder is for all the popular tricks implemented in RNN such as dropout, weight regularization, batch-normalize, etc. 

### Popular tricks
1. Dropout
	- [BayesianRNN][1]
	- [code][2]

2. Weight Normalization
	- [Accelerate DNN][3]
	- [code][4]

3. Boosting
	- [XGBoost][5]
	- [code][6]

4. lime
	- Explaining the predictions of any mahine learning classifier. 
	- [code][7]

5. persistent RNN
	- A technical report that accelerate RNN training with 30x speed. 
	- [report][8]

[1]:	http://arxiv.org/abs/1512.05287
[2]:	https://github.com/yaringal/BayesianRNN
[3]:	http://arxiv.org/abs/1602.07868
[4]:	https://github.com/TimSalimans/weight_norm
[5]:	http://arxiv.org/pdf/1603.02754.pdf
[6]:	https://github.com/dmlc/xgboost
[7]:	https://github.com/marcotcr/lime
[8]:	https://svail.github.io/persistent_rnns/